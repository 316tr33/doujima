#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Codexæ¤œè¨¼: æ±æµ·é“è‹±èªç‰ˆ Phase 2.10
ãƒ­ãƒ¼ãƒå­—é€£çµã®å®Œå…¨è‹±èªåŒ–ç¢ºèª
"""

import re
from pathlib import Path

en_html = Path('../Tokaido/en/stations.html').read_text(encoding='utf-8')

print('=' * 80)
print('æ±æµ·é“è‹±èªç‰ˆ Phase 2.10 è©³ç´°æ¤œè¨¼ï¼ˆå®Œå…¨è‹±èªåŒ–ï¼‰')
print('=' * 80)

all_ok = True
errors = []

# 1. ãƒ­ãƒ¼ãƒå­—é€£çµãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ¤œå‡º
print('\nã€1. ãƒ­ãƒ¼ãƒå­—é€£çµã®æ¤œå‡ºã€‘')

# å¤§æ–‡å­—å°æ–‡å­—ãŒé€£ç¶šã™ã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆãƒ­ãƒ¼ãƒå­—é€£çµã®ç‰¹å¾´ï¼‰
# ä¾‹: NihonbashiMuromachi, KawasakikuHonchou
english_content = re.search(r'<span class="english-address">([^<]+)</span>', en_html)
station_content = re.search(r'<span class="english-station">([^<]+)</span>', en_html)

# è‹±èªä½æ‰€å†…ã®ãƒ­ãƒ¼ãƒå­—é€£çµãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œç´¢
romaji_patterns = [
    r'[A-Z][a-z]+[A-Z][a-z]+[A-Z]',  # 3é€£ç¶šå¤§æ–‡å­—å§‹ã¾ã‚Šï¼ˆKawasakikuHonchouï¼‰
    r'[A-Z][a-z]{2,}[A-Z][a-z]{2,}[A-Z][a-z]{2,}',  # é•·ã„å˜èªã®é€£ç¶š
]

romaji_found = []
for pattern in romaji_patterns:
    matches = re.findall(pattern, en_html)
    for match in matches:
        # é™¤å¤–ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆæ­£å¸¸ãªè‹±èªè¡¨è¨˜ï¼‰
        if match in ['JRTokaido', 'JRYokohamaLine', 'JRTokaidoLine']:
            continue
        # HTML/JSã®ã‚­ãƒ£ãƒ¡ãƒ«ã‚±ãƒ¼ã‚¹ã‚‚é™¤å¤–
        if 'Content' in match or 'Element' in match or 'Loaded' in match:
            continue
        if match not in romaji_found:
            romaji_found.append(match)

if romaji_found:
    print(f'âœ— ãƒ­ãƒ¼ãƒå­—é€£çµæ¤œå‡º: {len(romaji_found)}ç®‡æ‰€')
    for match in romaji_found[:10]:
        print(f'  ç–‘ã‚ã—ã„: {match}')
    all_ok = False
    errors.append(f'ãƒ­ãƒ¼ãƒå­—é€£çµæ®‹å­˜: {len(romaji_found)}ç®‡æ‰€')
else:
    print('âœ“ ãƒ­ãƒ¼ãƒå­—é€£çµãªã—')

# 2. æ—¥æœ¬èªãƒ­ãƒ¼ãƒå­—æ¥å°¾è¾ã®æ¤œå‡º
print('\nã€2. æ—¥æœ¬èªæ¥å°¾è¾ã®æ¤œå‡ºã€‘')

# Phase 2.9ã§ä¿®æ­£ã™ã¹ãã ã£ãŸæ¥å°¾è¾
romaji_suffixes = [
    'Machi', 'Chou', 'Gun', 'Shi', 'Ku',
    'Ittai', 'Shuuhen', 'Kaiwai',
    'Riyou', 'Nado', 'Basu', 'Takushii'
]

suffix_found = {}
for suffix in romaji_suffixes:
    # å˜èªå¢ƒç•Œã§ã®å‡ºç¾ã‚’æ¤œç´¢ï¼ˆå¤§æ–‡å­—å§‹ã¾ã‚Šã®å˜èªã¨ã—ã¦ï¼‰
    pattern = rf'\b{suffix}\b'
    matches = re.findall(pattern, en_html)
    if matches:
        suffix_found[suffix] = len(matches)

if suffix_found:
    print(f'âœ— æ—¥æœ¬èªæ¥å°¾è¾æ¤œå‡º: {sum(suffix_found.values())}ç®‡æ‰€')
    for suffix, count in suffix_found.items():
        print(f'  {suffix}: {count}ç®‡æ‰€')
    all_ok = False
    errors.append(f'æ—¥æœ¬èªæ¥å°¾è¾æ®‹å­˜: {sum(suffix_found.values())}ç®‡æ‰€')
else:
    print('âœ“ æ—¥æœ¬èªæ¥å°¾è¾ãªã—')

# 3. å…¨è§’æ–‡å­—ã®æ¤œå‡ºï¼ˆè‹±èªã‚¨ãƒªã‚¢å†…ï¼‰
print('\nã€3. å…¨è§’æ–‡å­—ã®æ¤œå‡ºã€‘')

# è‹±èªéƒ¨åˆ†ï¼ˆenglish-address, english-stationï¼‰å†…ã®å…¨è§’æ–‡å­—
english_sections = re.findall(r'<span class="english-[^"]+">([^<]+)</span>', en_html)
fullwidth_found = []

for section in english_sections:
    # å…¨è§’æ‹¬å¼§
    if 'ï¼ˆ' in section or 'ï¼‰' in section:
        fullwidth_found.append(f'å…¨è§’æ‹¬å¼§: {section[:50]}...')
    # å…¨è§’è¨˜å·
    if 'â€»' in section:
        fullwidth_found.append(f'å…¨è§’â€»: {section[:50]}...')

if fullwidth_found:
    print(f'âœ— å…¨è§’æ–‡å­—æ¤œå‡º: {len(fullwidth_found)}ç®‡æ‰€')
    for item in fullwidth_found[:5]:
        print(f'  {item}')
    all_ok = False
    errors.append(f'å…¨è§’æ–‡å­—æ®‹å­˜: {len(fullwidth_found)}ç®‡æ‰€')
else:
    print('âœ“ å…¨è§’æ–‡å­—ãªã—')

# 4. æ„å‘³ä¸æ˜ãªè‹±èªã®æ¤œå‡º
print('\nã€4. æ„å‘³ä¸æ˜ãªè‹±èªè¡¨ç¾ã®æ¤œå‡ºã€‘')

# è‹±èªã¨ã—ã¦æ„å‘³ã‚’ãªã•ãªã„è¡¨ç¾
nonsense_patterns = {
    'shuuhen': 'area',
    'kaiwai': 'area',
    'ittai': 'area',
    'chuushinbu': 'central area',
    'riyou': 'use/available',
    'nado': 'etc.',
}

nonsense_found = {}
for pattern, correct in nonsense_patterns.items():
    matches = re.findall(pattern, en_html, re.IGNORECASE)
    if matches:
        nonsense_found[pattern] = len(matches)

if nonsense_found:
    print(f'âœ— æ„å‘³ä¸æ˜ãªè¡¨ç¾æ¤œå‡º: {sum(nonsense_found.values())}ç®‡æ‰€')
    for pattern, count in nonsense_found.items():
        print(f'  {pattern}: {count}ç®‡æ‰€')
    all_ok = False
    errors.append(f'æ„å‘³ä¸æ˜ãªè‹±èªæ®‹å­˜: {sum(nonsense_found.values())}ç®‡æ‰€')
else:
    print('âœ“ æ„å‘³ä¸æ˜ãªè¡¨ç¾ãªã—')

# 5. ã‚¹ãƒšãƒ¼ã‚¹åŒºåˆ‡ã‚Šã®ç¢ºèª
print('\nã€5. ã‚¹ãƒšãƒ¼ã‚¹åŒºåˆ‡ã‚Šã®ç¢ºèªã€‘')

# ä½æ‰€ãƒ»é§…åã®ã‚¹ãƒšãƒ¼ã‚¹åŒºåˆ‡ã‚ŠãŒé©åˆ‡ã‹
# ä¾‹: "Nihonbashi Muromachi" (è‰¯) vs "NihonbashiMuromachi" (æ‚ª)
address_samples = re.findall(r'<span class="english-address">([^<]+)</span>', en_html)
station_samples = re.findall(r'<span class="english-station">([^<]+)</span>', en_html)

poor_spacing = 0
for sample in address_samples[:10]:
    # å¤§æ–‡å­—ãŒé€£ç¶šã§ç¾ã‚Œã‚‹ï¼ˆã‚¹ãƒšãƒ¼ã‚¹ãªã—ï¼‰ãƒ‘ã‚¿ãƒ¼ãƒ³
    if re.search(r'[a-z]{3,}[A-Z][a-z]{3,}[A-Z]', sample):
        poor_spacing += 1

if poor_spacing > 0:
    print(f'âœ— ã‚¹ãƒšãƒ¼ã‚¹ä¸è¶³: {poor_spacing}ç®‡æ‰€ï¼ˆã‚µãƒ³ãƒ—ãƒ«10ä»¶ä¸­ï¼‰')
    all_ok = False
    errors.append(f'ã‚¹ãƒšãƒ¼ã‚¹åŒºåˆ‡ã‚Šä¸é©åˆ‡: {poor_spacing}ç®‡æ‰€')
else:
    print('âœ“ ã‚¹ãƒšãƒ¼ã‚¹åŒºåˆ‡ã‚Šé©åˆ‡')

# æœ€çµ‚çµæœ
print('\n' + '=' * 80)
if all_ok:
    print('ğŸ‰ Codexæ¤œè¨¼ Phase 2.10: 100ç‚¹æº€ç‚¹ï¼')
    print('ğŸ“Š å®Œå…¨è‹±èªåŒ–ãŒå®Œç’§ã«å®Œäº†ã—ã¦ã„ã¾ã™')
    print('\nã€æ¤œè¨¼åˆæ ¼é …ç›®ã€‘')
    print('  âœ“ ãƒ­ãƒ¼ãƒå­—é€£çµ: å®Œå…¨è§£æ¶ˆ')
    print('  âœ“ æ—¥æœ¬èªæ¥å°¾è¾: å®Œå…¨è‹±èªåŒ–')
    print('  âœ“ å…¨è§’æ–‡å­—: å®Œå…¨é™¤å»ï¼ˆè‹±èªéƒ¨åˆ†ï¼‰')
    print('  âœ“ æ„å‘³ä¸æ˜ãªè¡¨ç¾: å®Œå…¨è‹±èªåŒ–')
    print('  âœ“ ã‚¹ãƒšãƒ¼ã‚¹åŒºåˆ‡ã‚Š: é©åˆ‡')
    print('\nã€æˆæœã€‘')
    print('  - ä½æ‰€: 69ç®‡æ‰€ã®é€£çµãƒ­ãƒ¼ãƒå­—ã‚’è‹±èªåŒ–')
    print('  - é§…å: 31ç®‡æ‰€ã®é€£çµãƒ­ãƒ¼ãƒå­—ã‚’è‹±èªåŒ–')
    print('  - å…¨è§’æ–‡å­—: 260ç®‡æ‰€ã‚’åŠè§’ã«å¤‰æ›')
    print('  - è‹±èªãƒã‚¤ãƒ†ã‚£ãƒ–ãŒç†è§£å¯èƒ½ãªè¡¨ç¾ã«çµ±ä¸€')
    print('=' * 80)
    exit(0)
else:
    print(f'âŒ Codexæ¤œè¨¼ Phase 2.10: {len(errors)}ä»¶ã®ã‚¨ãƒ©ãƒ¼')
    print('\nã€ã‚¨ãƒ©ãƒ¼ä¸€è¦§ã€‘')
    for i, error in enumerate(errors, 1):
        print(f'{i}. {error}')
    print('=' * 80)
    exit(1)
